{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example of a fully connected deep neural network (FCN)\n",
    "# The FCN trains on images of handwritten numbers from 0 to 9, then\n",
    "# takes an image of a handwritten number and estimates which number\n",
    "# it represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib to display digit images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch data used in example program\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as xforms\n",
    "from torch.autograd import Variable\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':[8,6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  # The image size = 28x28 = 784\n",
    "hidden_size = 500  # Number of nodes in the hidden layer\n",
    "num_classes = 10  # Number of output classes (0..9)\n",
    "num_epochs = 5  # Number of times DNN is trained on training dataset\n",
    "batch_size = 100  # The size of input data for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read om training and test datasets, these are included as part of\n",
    "# PyTorch's dataset repository\n",
    "\n",
    "train_dataset = dsets.MNIST( root ='./data', train=True, transform=xforms.ToTensor(), download=True )\n",
    "test_dataset = dsets.MNIST( root='./data', train=False, transform=xforms.ToTensor() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2cf49e48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at two of the images in the training dataset, and the\n",
    "# corresponding label (which defines the number the image represents)\n",
    "\n",
    "# Grab the first two images and corresponding numbers they represent\n",
    "# from the training set\n",
    "\n",
    "img_0, val_0 = train_dataset[ 0 ]\n",
    "img_1, val_1 = train_dataset[ 1 ]\n",
    "\n",
    "# Print out what numbers the two images are meant to represent\n",
    "\n",
    "print( val_0, '  ', val_1 )\n",
    "\n",
    "# Create a single row of two images, place first image in the first\n",
    "# image position\n",
    "\n",
    "plt.subplot( 1, 2, 1 )\n",
    "plt.imshow( img_0.numpy()[ 0 ], cmap='gray' )\n",
    "\n",
    "# Place the second image in the second image position\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "plt.imshow( img_1.numpy()[ 0 ], cmap='gray' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders, these will load and cache data of batch_size during\n",
    "# looping, training data is shuffled randomly, test data is not\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( dataset=train_dataset, batch_size=batch_size, shuffle=True )\n",
    "test_loader = torch.utils.data.DataLoader( dataset=test_dataset, batch_size=batch_size, shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Linear( input_size, hidden_size ),  # fully connected layer, 784 (input data) -> 500 (hidden layer)\n",
    "  nn.ReLU(),  # Rectified liner unit filter, max( 0, x )\n",
    "  nn.Linear( hidden_size, num_classes )  # output layer, 500 (hidden layer) -> 10 (output classes)\n",
    ")\n",
    "\n",
    "#net.cuda()  # Uncomment to enable GPU computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cross-entropy loss function to use during evaluation of DNN performance, and an\n",
    "# Adam optimizer (Adam, see https://arxiv.org/abs/1412.6980) to update DNN weights\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam( net.parameters(), lr=learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.2831\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1667\n",
      "Epoch [1/5], Step [300/600], Loss: 0.4066\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2115\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1525\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1872\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1301\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0673\n",
      "Epoch [2/5], Step [300/600], Loss: 0.2141\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1981\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1679\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1076\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1339\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0599\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0722\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0670\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0726\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0932\n",
      "Epoch [4/5], Step [100/600], Loss: 0.1119\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0359\n",
      "Epoch [4/5], Step [300/600], Loss: 0.1107\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0520\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0197\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0838\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0355\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0267\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0308\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0299\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0634\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0209\n"
     ]
    }
   ],
   "source": [
    "# Train the DNN\n",
    "\n",
    "for epoch in range( 0, num_epochs ):  # For each of the five epochs\n",
    "    \n",
    "    for i, (images, labels) in enumerate( train_loader ):  # For each of the 100-image batches\n",
    "        \n",
    "        images = Variable( images.view( -1, 28 * 28 ))  # Convert torch tensor to Variable, from 784-pix image to 28x28 matrix\n",
    "        labels = Variable( labels )  # Grab pre-assigned labels for each of the 100 images\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero gradients for current pass\n",
    "        outputs = net( images )  # Forward pass, compute output class for given image\n",
    "        loss = criterion( outputs, labels )  # Compute loss, difference between estimated label and actual label\n",
    "        loss.backward()  # Backpropegation, compute improved weights\n",
    "        optimizer.step()  # Update weights of hidden nodes\n",
    "        \n",
    "        if ( i + 1 ) % 100 == 0:  # Log message\n",
    "            print( 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % ( epoch + 1, num_epochs, i + 1, len( train_dataset ) // batch_size, loss.data ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10K test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Test performance on test images\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:  # For each image the the test dataset\n",
    "    \n",
    "    images = Variable( images.view( -1, 28 * 28 ) )  # Convert torch tensor to Variable, from 784-pix image to 28x28 matrix\n",
    "    outputs = net( images )  # Get correct lables for each image\n",
    "    \n",
    "    _, predicted = torch.max( outputs.data, 1 )  # Choose best class from DNN output\n",
    "    total += labels.size( 0 )  # Increment total count\n",
    "    correct += ( predicted == labels ).sum()  # Increment correct prediction count\n",
    "    \n",
    "# Print overall accuracy of labeling\n",
    "    \n",
    "print( 'Accuracy on 10K test images: %d %%' % ( 100 * correct / total ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
